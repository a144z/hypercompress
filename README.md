# Hypercompress

**Extreme-scale neural network compression toolkit** targeting up to **1000Ã— effective model size reduction** through hybrid compression techniques. Hypercompress combines low-rank approximation (LRA), KV-cache distillation, byte latent transformers (BLT), and aggressive sparsification in a unified, production-ready pipeline.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ Key Features

- **ğŸ§¬ Hybrid Compression Pipeline**: Modular branches (LRA, KV-distill, BLT, sparsity) run concurrently with configurable orchestration
- **ğŸ“Š Adaptive Rank Estimation**: ARSVD with guard-railed thresholds per attention and MoE block
- **ğŸ¯ 1000Ã— Compression Target**: Automatic tracking and per-branch ratio attribution toward extreme compression goals
- **ğŸ‘¨â€ğŸ« Teacherâ€“Student Distillation**: Hierarchical LoRA updates with activation alignment
- **ğŸ”„ Iterative Fine-Tuning**: ReLoRA updates with Adapprox optimizers
- **ğŸ“ˆ Comprehensive Evaluation**: Built-in harness for PPL, MMLU, GSM8K, GLUE, and branch ablations
- **ğŸ“¦ Artifact Management**: Integrated Weights & Biases and TensorBoard logging
- **ğŸ³ Production Ready**: Batteries-included Docker images, Taskfile automation, and comprehensive test suite

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Compression Techniques](#compression-techniques)
- [Configuration](#configuration)
- [Usage Examples](#usage-examples)
- [Project Structure](#project-structure)
- [Testing](#testing)
- [Completed Tasks](#completed-tasks)
- [To Do List](#to-do-list)
- [Contributing](#contributing)
- [License](#license)

## ğŸ”§ Installation

### Prerequisites

- Python 3.10 or higher
- PyTorch 2.2+ (with CUDA support recommended)
- For Hugging Face models: `transformers` and `accelerate`

### Install Hypercompress

```bash
# Using pip
pip install -e .

# Or using uv (recommended, faster)
uv pip install -e .

# With development tools (tests, linting)
pip install -e ".[dev]"
```

### Verify Installation

```bash
python scripts/quick_test.py
```

This runs a minimal compression pipeline with synthetic data to ensure everything works correctly.

## ğŸ¯ Quick Start

### Basic Example (Synthetic Data)

Run the compression pipeline with a sample configuration:

```bash
python scripts/run_pipeline.py --config configs/sample_hybrid.yaml --ratio 100
```

### Target 1000Ã— Compression

```bash
python scripts/run_pipeline.py --config configs/default.yaml --ratio 1000
```

### Real-World LLM Compression

Compress large language models from Hugging Face:

```bash
python scripts/run_hf_pipeline.py \
  --config configs/hf_12b.yaml \
  --ratio 100 \
  --teacher-model meta-llama/Meta-Llama-3-12B \
  --student-model meta-llama/Meta-Llama-3-8B-Instruct \
  --distill-data data/wikipedia.txt \
  --finetune-data data/instruct_mix.txt \
  --seq-len 2048 \
  --stride 1024 \
  --batch-size 1
```

**Quick dataset setup:**

```bash
mkdir -p data
curl -L https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/distill.txt
cp data/distill.txt data/finetune.txt
```

**Public model example (no auth required):**

```bash
python scripts/run_hf_pipeline.py \
  --config configs/hf_example_public.yaml \
  --teacher-model microsoft/phi-2 \
  --student-model microsoft/phi-1_5 \
  --distill-data data/distill.txt \
  --seq-len 2048 \
  --stride 1024
```

For detailed instructions, see [QUICKSTART.md](QUICKSTART.md).

## ğŸ§ª Compression Techniques

Hypercompress implements a multi-stage hybrid compression pipeline:

### 1. Low-Rank Approximation (LRA)
Factorizes weight matrices using singular value decomposition (SVD) to reduce parameter count while preserving performance.

### 2. KV-Cache Distillation
Compresses key-value cache projections to reduce memory footprint during inference.

### 3. Byte Latent Transformers (BLT)
Structural compression of embedding layers by replacing high-dimensional embeddings with low-dimensional latent representations.

### 4. Sparsity Acceleration
Prunes weights to achieve extreme sparsity ratios (up to 99%+) while maintaining model accuracy.

### 5. Structural Compression
Replaces linear layers with factorized equivalents to achieve true parameter reduction.

### 6. Knowledge Distillation
Teacher-student training transfers knowledge from large models to compressed students.

## âš™ï¸ Configuration

Compression runs are configured via YAML files. Example:

```yaml
mode: hybrid
targets:
  compression_ratio: 1000
  retained_accuracy: 0.95
branches:
  lra:
    rank: 64
    sparsity: 0.99
  kv:
    cache_tokens: 128
  blt:
    embedding_reduction: 0.9
  sparsity:
    target_sparsity: 0.99
logging:
  experiment: sample-hybrid
```

See `configs/default.yaml` for a complete reference of all available options.

## ğŸ“Š Compression Reporting

Every pipeline run produces a `CompressionSummary` that aggregates:

- Branch-level compression ratios
- Parameter statistics (before/after)
- Target compliance tracking
- Per-branch attribution metrics

Results are automatically logged to TensorBoard, Weights & Biases, or custom dashboards under the `compression/*` namespace.

## ğŸ“ Project Structure

```
hypercompress/
â”œâ”€ src/distilled_kv/          # Main package (NOTE: directory name will be updated)
â”‚  â”œâ”€ pipeline.py             # Main orchestrator
â”‚  â”œâ”€ config.py               # Typed configuration loading
â”‚  â”œâ”€ logging.py              # Structured logging utilities
â”‚  â”œâ”€ modules/                # Compression branches
â”‚  â”‚  â”œâ”€ lra.py
â”‚  â”‚  â”œâ”€ kv_distill.py
â”‚  â”‚  â”œâ”€ blt.py
â”‚  â”‚  â”œâ”€ sparsity.py
â”‚  â”‚  â””â”€ merge.py
â”‚  â”œâ”€ structural/             # Structural compression utilities
â”‚  â”œâ”€ utils/                   # Shared math + helpers
â”‚  â”œâ”€ analysis/                # Compression summaries and reports
â”‚  â”œâ”€ distillation/           # Teacher-student distillation
â”‚  â”œâ”€ finetune/                # Fine-tuning loops
â”‚  â”œâ”€ evaluation/              # Evaluation harness
â”‚  â””â”€ storage/                 # Artifact management
â”œâ”€ configs/                    # YAML configuration presets
â”œâ”€ scripts/                     # Runnable entry points
â”œâ”€ tests/                       # Unit tests (pytest)
â”œâ”€ docker/                      # Containerized training environment
â”œâ”€ Taskfile.yml                # Automation helpers
â””â”€ pyproject.toml              # Packaging metadata
```

## ğŸ§ª Testing

Run the test suite:

```bash
pytest
```

Run with coverage:

```bash
pytest --cov=distilled_kv --cov-report=html
```

## ğŸ“š Advanced Topics

For advanced research directions (ZipNN, chained hierarchical LoRA, recursive SSMs, etc.) that can be incorporated into custom branches, see `docs/1000x-playbook.md`.

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Run tests and linting: `Task lint`
4. Commit your changes (`git commit -m 'Add amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

Please ensure new features include:

- Test coverage
- Documentation updates
- Type hints where applicable

## ğŸ“ License

MIT Â© 2025 Hypercompress Contributors

## âœ… Completed Tasks

- âœ… Renamed repository from "Distilled KV" to "Hypercompress"
- âœ… Updated README.md with improved structure, badges, and comprehensive documentation
- âœ… Updated `pyproject.toml` with new package name "hypercompress"
- âœ… Updated package docstrings and references to reflect new name
- âœ… Configured comprehensive `.gitignore` to ignore all model files (`.pt`, `.bin`, `.pth`, `.safetensors`, etc.)
- âœ… Configured `.gitignore` to ignore artifacts directories (`.artifacts/`, `artifacts/`, `checkpoints/`)
- âœ… Removed large model files (380+ MB) from git tracking
- âœ… Verified artifacts directory is properly ignored by Git
- âœ… Cleaned up repository cache and verified clean state

## ğŸ“ To Do List

### High Priority
- [ ] Rename package directory from `distilled_kv` to `hypercompress` (requires updating all imports)
- [ ] Update all internal references and documentation to use new package name
- [ ] Add CI/CD pipeline for automated testing
- [ ] Add comprehensive examples in `examples/` directory

### Medium Priority
- [ ] Create detailed API documentation with Sphinx or MkDocs
- [ ] Add more compression technique implementations
- [ ] Benchmark compression performance on standard datasets
- [ ] Improve error handling and user-friendly error messages

### Low Priority
- [ ] Add Docker Compose setup for easy development environment
- [ ] Create video tutorials or interactive notebooks
- [ ] Add support for more model formats (ONNX, TensorFlow)
- [ ] Implement distributed training support

## ğŸ™ Acknowledgments

Built with cutting-edge research in neural network compression, knowledge distillation, and efficient transformer architectures.
